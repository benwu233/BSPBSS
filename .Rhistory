od5 = matrix(0, nrow = 5, ncol = 6)
for(i in 1:6){
od5[,i] = order(cor1[,i],decreasing = TRUE)[1:5]
}
od10[,c(3,2,1,6,5,4)]
od5[,c(3,2,1,6,5,4)]
cor1
cor1 = matrix(0,nrow = 90, ncol = 6)
for(j in 1:90){
for(i in 1:3){
cor1[j,i] = mean(prob[[i]][(nii==j)&&(b[[i]]<0)] )
}
for(i in 4:6){
cor1[j,i] = mean(prob_inter[[i-3]][(nii==j)&&(b_inter[[i-3]]<0)] )
}
}
od5 = matrix(0, nrow = 5, ncol = 6)
for(i in 1:6){
od5[,i] = order(cor1[,i],decreasing = TRUE)[1:5]
}
od5[,c(3,2,1,6,5,4)]
b[[i]]<0
b[[1]]<0
b[[1]]
b[[1]][1]
b[[1]][10]
b[[1]][100]
table(b[[1]])
(nii==j)&&(b[[i]]<0)
(nii==j)&(b[[i]]<0)
(nii==j)&(b[[1]]<0)
cor1 = matrix(0,nrow = 90, ncol = 6)
for(j in 1:90){
for(i in 1:3){
cor1[j,i] = mean(prob[[i]][(nii==j)&(b[[i]]<0)] )
}
for(i in 4:6){
cor1[j,i] = mean(prob_inter[[i-3]][(nii==j)&(b_inter[[i-3]]<0)] )
}
}
od5 = matrix(0, nrow = 5, ncol = 6)
for(i in 1:6){
od5[,i] = order(cor1[,i],decreasing = TRUE)[1:5]
}
od5[,c(3,2,1,6,5,4)]
my_write_nii(path0,"test2b0b_n",tmp,nii)
cor1[,3]
od10[,3]
b[[3]][nii==40]
sum( b[[3]][nii==40] <0)
sum( b[[3]][nii==40] >0)
sum( b[[3]][nii==23] >0)
sum( b[[3]][nii==23] <0)
b[[3]][nii==23]
b[[3]][nii==23]*prob[[3]][nii==23 ]
sum( b[[3]][nii==23]*prob[[3]][nii==23 ] <0)
sum( b[[3]][nii==23]*prob[[3]][nii==23 ] >0)
sum( b[[3]][nii==23] <0)
sum( b[[3]][nii==23] >0)
od10[,6]
cor0[,6]
cor0[order(cor0[,6],decreasing=TRUE),6]
library(torch)
library(torchvision)
library(BNN)
yind1 = 4
yind2 = 7
library(BNNSTGP)
library(lattice)
library(gridExtra)
library(torch)
library(torchvision)
library(BNN)
load("/Users/ben/Desktop/work2/BNNSTGP/data/index.RData")
load("/Users/ben/Desktop/work2/BNNSTGP/data/mnist_idx.RData")
mnist = readRDS("/Users/ben/Desktop/work2/BNNSTGP/data/mnist.rds")
f_mnist = readRDS(file = "/Users/ben/Desktop/work2/BNNSTGP/data/f_mnist.rds")
source("/Users/ben/Desktop/work2/BNNSTGP/script/func.R")
#read index #mnist
idx_list = list()
idx_list[[1]] = idx35
idx_list[[2]] = idx38
idx_list[[3]] = idx47
idx_list[[4]] = idx49
yind = cbind(c(3,5),c(3,8),c(4,7),c(4,9))
xgrid = as.matrix(expand.grid(1:28,1:28))
data0 = mnist
yind1 = 4
yind2 = 7
tmp1 = get_test_data(data0$train,yind1)
tmp2 = get_test_data(data0$train,yind2)
x_test0 = rbind( tmp1$x, tmp2$x)/3
y_test0 = c(tmp1$y,tmp2$y)
n_test = length(y_test0)
dim(y_test0) = c(n_test,1)
x_test = get_2D(x_test0,xgrid)
dim(x_test) = c(n_test,1,28,28)
x1 = get_2D(tmp1$x,xgrid)
x2 = get_2D(tmp2$x,xgrid)
kk=3
BNNres = readRDS(paste0(file = "/Users/ben/Desktop/work2/BNNSTGP/result_new/BNN",yind1,yind2,"_allseed11",kk,".rds"))
length(BNNres$mar)
fig = levelplot( (x2[1,,,1]/255 + BNNres$mar[-1])/2, col.regions = gray(0:100/100),
xlab="", ylab="", main = paste('Digit', yind2),
scales=list(x=list(at=NULL), y=list(at=NULL)),
at=seq(0, 1, length.out=100))
plot(fig)
?BNNsel
fig = levelplot( (x2[1,,,1]/255 + BNNres$mar[-1])/2, col.regions = gray(0:100/100),
xlab="", ylab="", main = paste('Digit', yind2),
scales=list(x=list(at=NULL), y=list(at=NULL)),
at=seq(0, 1, length.out=100))
plot(fig)
dim(x2[1,,,1])
fig = levelplot( (x1[1,,,1]/255 + BNNres$mar[-1])/2, col.regions = gray(0:100/100),
xlab="", ylab="", main = paste('Digit', yind2),
scales=list(x=list(at=NULL), y=list(at=NULL)),
at=seq(0, 1, length.out=100))
plot(fig)
fig = levelplot( (x2[1,,,1]/255 + BNNres$mar[-1])/2, col.regions = gray(0:100/100),
xlab="", ylab="", main = paste('Digit', yind2),
scales=list(x=list(at=NULL), y=list(at=NULL)),
at=seq(0, 1, length.out=100))
plot(fig)
tmp = get_data(data0$train, ind0)
data0 = mnist
yind1 = 4
yind2 = 7
tmp1 = get_test_data(data0$test,yind1)
tmp2 = get_test_data(data0$test,yind2)
x_test0 = rbind( tmp1$x, tmp2$x)/255
y_test0 = c(tmp1$y,tmp2$y)
ind0 = which( ((data0$train$y==yind1) + (data0$train$y==yind2) ) >0 )
tmp = get_data(data0$train, ind0)
x_train0 = tmp$x/255
dim(x_train0)
library(BSPBSS)
library(ica)
library(oro.nifti)
library(neurobase)
library(coda)
library(RandomFieldsUtils)
library(PrevMap)
load("/Users/ben/desktop/work3/data/abide_3mm.RData")
d0 = 0.5
mask1 = rep(0, nrow(coord))
coord0 = as.matrix(coord)
for(i in 1:nrow(coord)){
mask1[i] = aal_map[coord0[i,][1],coord0[i,][2],coord0[i,][3]]
}
rm(falff)
rm(lfcd)
rm(reho)
gc()
set.seed(612)
ini = init_bspbss(degree_weighted, mask = (mask1>0), xgrid = coord, dens = d0, q = 10, kernel="matern",ker_par = c(1/10,5/2), num_eigen = 500 )
path0 = "/Users/ben/desktop/work2/GaussianProcess/real_kernel/"
saveRDS(ini,paste0(path0,"ini_mk01_5.rds") )
n = 10*10
ind = sample(n,10)
theta = diag(1,n,n)
theta[ind] = theta[ind]+ 0.2
theta
diag(1,n,n)
diag(1,n,n)[1,1]
diag(1,n,n)[2,2]
sum(theta!=0)
diag(1,n)
dim(diag(1,n))
n = 10*10
ind = sample(n,10)
theta = diag(1,10)
theta[ind] = theta[ind]+ 0.2
theta
ind1 = sample(n,15)
n = 10*10
ind = sample(n,10)
theta0 = diag(1,10)
theta1 = theta0
theta1 = theta1 + theta1[ind] * 0.3
ind1 = sample(n,15)
theta2 = theta0
theta2 = theta2 + theta2[ind1] * 0.3
n = 10*10
ind = sample(n,10)
theta0 = diag(1,10)
theta1 = theta0
theta1[ind] = theta1[ind] + theta1[ind] * 0.3
ind1 = sample(n,15)
theta2 = theta0
theta2[ind1] = theta2[ind1] + theta2[ind1] * 0.3
inv( inv(theta1) * 0.5 +  inv(theta2) * 0.8 )
solve( solve(theta1) * 0.5 +  solve(theta2) * 0.8 )
nii = readNIfTI('/Users/Ben/Desktop/work2/GaussianProcess/AAL_90_3mm.nii')
path0 = "/Users/ben/desktop/work2/GaussianProcess/real_kernel/"
savepath = "/Users/ben/desktop/work2/GaussianProcess/real_kernel/res/"
out = ini$init$S
file0 = paste0(savepath ,'mk01_5ini')
output_nii(out,nii,coord,mask1>0,file0,std=TRUE)
library(BSPBSS)
library(ica)
library(oro.nifti)
library(neurobase)
library(coda)
library(RandomFieldsUtils)
library(PrevMap)
MClength = 5000
burn_in = 4000
show_step = 1000
X_list = readRDS("/Users/ben/Desktop/work2/GaussianProcess/sim_kernel2/Xlist5e4s0.rds")
X = X_list[[1]]
ini = init_bspbss(X$X, xgrid = X$xgrid, q = 3,dens= 0.5, kernel="matern",ker_par = c(1/20,3/2), num_eigen = 50 )
res = mcmc_bspbss(X$X,ini$init,ini$prior,ini$kernel,ep=0.1,lr = 0.1,decay = 0.1,subsample_n = 1, subsample_p = 1,MClength,burn_in,thin=1,show_step)
res_sum = sum_mcmc_bspbss(res, X$X, ini$kernel, start = 1, end = 1000, select_p = 0.5)
savepath  = "/Users/ben/Desktop/work2/GaussianProcess/round1"
max(abs(ini$init$S))
max(abs(res_sum$S) )
levelplot2D(S=res_sum$S*100, lim = c(-4,4),xgrid = X$xgrid,path = savepath,name = "test1")
levelplot2D(S=ini$init$S*100, lim = c(-4,4),xgrid = X$xgrid,path = savepath,name = "test1")
library(BSPBSS)
library(BSPBSS)
library(BSPBSS)
library(ica)
library(oro.nifti)
library(neurobase)
library(coda)
library(RandomFieldsUtils)
library(PrevMap)
MClength = 5000
burn_in = 4000
show_step = 1000
X = X_list[[1]]
ini = init_bspbss(X$X, xgrid = X$xgrid, q = 3,dens= 0.5, kernel="matern",ker_par = c(1/20,3/2), num_eigen = 50 )
ini = init_bspbss(X$X, xgrid = X$xgrid, q = 3,dens= 0.5, kernel="matern",ker_par = c(1/20,3/2), num_eigen = 50 )
library(BSPBSS)
res = mcmc_bspbss(X$X,ini$init,ini$prior,ini$kernel,ep=0.1,lr = 0.1,decay = 0.1,subsample_n = 1, subsample_p = 1,MClength,burn_in,thin=1,show_step)
res_sum = sum_mcmc_bspbss(res, X$X, ini$kernel, start = 1, end = 1000, select_p = 0.5)
savepath  = "/Users/ben/Desktop/work2/GaussianProcess/round1"
max(abs(res_sum$S) )
levelplot2D(S=ini$init$S*100, lim = c(-4,4),xgrid = X$xgrid,path = savepath,name = "test1")
levelplot2D(S=res_sum$S*100, lim = c(-4,4),xgrid = X$xgrid,path = savepath,name = "test1")
mnist_trainset=read_csv("/Users/ben/desktop/mnist_train.csv",col_names = FALSE)
rm(list=ls())
library(torch)
library(readr)
library(dplyr)
mnist_trainset=read_csv("/Users/ben/desktop/mnist_train.csv",col_names = FALSE)
#mnist_testset=read_csv("E:\\data\\mnist\\mnist_test.csv",col_names = FALSE)
mnist_testset=read_csv("/Users/ben/desktop/mnist_test.csv",col_names = FALSE)
print(dim(mnist_testset))
trainX=mnist_trainset[,2:785]/255.0#标准化
trainY=mnist_trainset[,1]
testX=mnist_testset[,2:785]/255.0#标准化一下
testY=mnist_testset[,1]
library(torch)
library(data.table)
library(mltools)#one_hot编码
mnist_dataset <- dataset(
"mnist_dataset",
# the input data to your dataset goes in the initialize function.
# our dataset will take a dataframe and the name of the response
# variable.
initialize = function(df, response_variable) {
self$df <- df[,-which(names(df) == response_variable)]
label <-data.table(digit= as.factor(df[[response_variable]]))
self$response_variable=as.matrix(one_hot(label))
},
# onehotEncode=function(label){
#   new_code=torch_zeros(dim(self$df)[1],length(unique(label)))
#
#   for(i in c(1:dim(self$df)[1])){
#     if(label[i]!=0){
#     new_code[i,label[i]]=1
#     }else{
#       new_code[i,length(unique(label))]=1
#     }
#   }
#   return(new_code)
# },
# the .getitem method takes an index as input and returns the
# corresponding item from the dataset.
# the index could be anything. the dataframe could have many
# rows for each index and the .getitem method would do some
# kind of aggregation before returning the element.
# in our case the index will be a row of the data.frame,
.getitem = function(index) {
#browser()
response <- torch_tensor(self$response_variable[index,])
x <- torch_tensor(as.numeric(self$df[index,]))
# note that the dataloaders will automatically stack tensors
# creating a new dimension
list(x = x, y = response)
},
# It's optional, but helpful to define the .length method returning
# the number of elements in the dataset. This is needed if you want
# to shuffle your dataset.
.length = function() {
dim(self$response_variable)[1]
}
)
colnames(trainY)="label"
train=cbind(trainX,trainY)
train_dataset <- mnist_dataset(train, "label")
train_dataset$.getitem(1)
colnames(testY)="label"
test=cbind(testX,testY)
test_dataset <- mnist_dataset(test, "label")
train_loader <- dataloader(train_dataset, batch_size = 2, shuffle = TRUE)
test_loader=dataloader(test_dataset,batch_size = 2,shuffle = TRUE)
net <- nn_module(
"mnist_classification",
initialize = function() {
self$hidden1=nn_linear(784,30)
self$output=nn_linear(30,10)
self$softmax=nn_softmax(10)
},
forward = function(x) {
x=self$hidden1(x)
x=torch_relu(x)
x=self$output(x)
x=self$softmax(x)
}
)
model=net()
optimizer=optim_sgd(model$parameters,lr=1e-2)
n_epoch=4
epoch = 1
enumerate(train_loader)
dim(enumerate(train_loader))
length(enumerate(train_loader))
batch = enumerate(train_loader)[1]
browser()
optimizer$zero_grad()
input=batch[[1]]
out=model(input)
loss=nnf_cross_entropy(out,batch[[2]])
loss$backward()
optimizer$step()
cat(sprintf("Loss at epoch %d: %3f\n", epoch, loss$item()))
for(batch in enumerate(train_loader)) {
browser()
optimizer$zero_grad()
input=batch[[1]]
out=model(input)
loss=nnf_cross_entropy(out,batch[[2]])
loss$backward()
optimizer$step()
cat(sprintf("Loss at epoch %d: %3f\n", epoch, loss$item()))
}
for(epoch in 1:n_epoch){
for(batch in enumerate(train_loader)) {
browser()
optimizer$zero_grad()
input=batch[[1]]
out=model(input)
loss=nnf_cross_entropy(out,batch[[2]])
loss$backward()
optimizer$step()
cat(sprintf("Loss at epoch %d: %3f\n", epoch, loss$item()))
}
}
model=net()
optimizer=optim_sgd(model$parameters,lr=1e-2)
n_epoch=4
for(epoch in 1:n_epoch){
for(batch in enumerate(train_loader)) {
browser()
optimizer$zero_grad()
input=batch[[1]]
out=model(input)
loss=nnf_cross_entropy(out,batch[[2]])
loss$backward()
optimizer$step()
cat(sprintf("Loss at epoch %d: %3f\n", epoch, loss$item()))
}
}
model=net()
optimizer=optim_sgd(model$parameters,lr=1e-2)
n_epoch=4
for(epoch in 1:n_epoch){
for(batch in enumerate(train_loader)) {
browser()
optimizer$zero_grad()
input=batch[[1]]
out=model(input)
loss=nnf_cross_entropy(out,batch[[2]])
loss$backward()
optimizer$step()
cat(sprintf("Loss at epoch %d: %3f\n", epoch, loss$item()))
}
}
model=net()
optimizer=optim_sgd(model$parameters,lr=1e-2)
n_epoch=4
for(epoch in 1:n_epoch){
for(batch in enumerate(train_loader)) {
#browser()
optimizer$zero_grad()
input=batch[[1]]
out=model(input)
loss=nnf_cross_entropy(out,batch[[2]])
loss$backward()
optimizer$step()
cat(sprintf("Loss at epoch %d: %3f\n", epoch, loss$item()))
}
}
rm(list=ls())
rm(list=ls())
library(torch)
library(readr)
library(dplyr)
_trainset=read_csv("E:\\data\\mnist\\mnist_train.csv",col_names = FALSE)
mnist_trainset=read_csv("/Users/ben/desktop/mnist_train.csv",col_names = FALSE)
print(dim(mnist_trainset))
#mnist_testset=read_csv("E:\\data\\mnist\\mnist_test.csv",col_names = FALSE)
mnist_testset=read_csv("/Users/ben/desktop/mnist_test.csv",col_names = FALSE)
print(dim(mnist_testset))
trainX=mnist_trainset[,2:785]/255.0#标准化
trainY=mnist_trainset[,1]
testX=mnist_testset[,2:785]/255.0#标准化一下
testY=mnist_testset[,1]
library(torch)
library(data.table)
library(mltools)#one_hot编码
mnist_dataset <- dataset(
"mnist_dataset",
# the input data to your dataset goes in the initialize function.
# our dataset will take a dataframe and the name of the response
# variable.
initialize = function(df, response_variable) {
self$df <- df[,-which(names(df) == response_variable)]
label <-data.table(digit= as.factor(df[[response_variable]]))
self$response_variable=as.matrix(one_hot(label))
},
# onehotEncode=function(label){
#   new_code=torch_zeros(dim(self$df)[1],length(unique(label)))
#
#   for(i in c(1:dim(self$df)[1])){
#     if(label[i]!=0){
#     new_code[i,label[i]]=1
#     }else{
#       new_code[i,length(unique(label))]=1
#     }
#   }
#   return(new_code)
# },
# the .getitem method takes an index as input and returns the
# corresponding item from the dataset.
# the index could be anything. the dataframe could have many
# rows for each index and the .getitem method would do some
# kind of aggregation before returning the element.
# in our case the index will be a row of the data.frame,
.getitem = function(index) {
#browser()
response <- torch_tensor(self$response_variable[index,])
x <- torch_tensor(as.numeric(self$df[index,]))
# note that the dataloaders will automatically stack tensors
# creating a new dimension
list(x = x, y = response)
},
# It's optional, but helpful to define the .length method returning
# the number of elements in the dataset. This is needed if you want
# to shuffle your dataset.
.length = function() {
dim(self$response_variable)[1]
}
)
colnames(trainY)="label"
train=cbind(trainX,trainY)
train_dataset <- mnist_dataset(train, "label")
train_dataset$.getitem(1)
colnames(testY)="label"
test=cbind(testX,testY)
test_dataset <- mnist_dataset(test, "label")
train_loader <- dataloader(train_dataset, batch_size = 2, shuffle = TRUE)
test_loader=dataloader(test_dataset,batch_size = 2,shuffle = TRUE)
#建立模型
net <- nn_module(
"mnist_classification",
initialize = function() {
self$hidden1=nn_linear(784,30)
self$output=nn_linear(30,10)
self$softmax=nn_softmax(10)
},
forward = function(x) {
x=self$hidden1(x)
x=torch_relu(x)
x=self$output(x)
x=self$softmax(x)
}
)
model=net()
optimizer=optim_sgd(model$parameters,lr=1e-2)
n_epoch=4
for(epoch in 1:n_epoch){
for(batch in enumerate(train_loader)) {
#browser()
optimizer$zero_grad()
input=batch[[1]]
out=model(input)
loss=nnf_cross_entropy(out,batch[[2]])
loss$backward()
optimizer$step()
cat(sprintf("Loss at epoch %d: %3f\n", epoch, loss$item()))
}
}
tanh(1)
tanh(10)
tanh(-10)
tanh()
tanh(0)
1/(1+exp(-10))
